import random
import statistics
import networkx as nx
import pandas as pd
from itertools import combinations
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

#G.add_edges_from(seed[0])
#G=nx.watts_strogatz_graph(25,4,0.2)
#G=nx.fast_gnp_random_graph(10,1)




num = 30
samples = 1000
steps=250
title='Clique Network'


influence=[]




ultravertical = nx.DiGraph()
for n in range(num):
    ultravertical.add_edge(n, n + 1)

F=ultravertical



def test():
    l=0

    def networkgeneration():
        G = nx.barabasi_albert_graph(num, 2)
        if nx.is_connected(G):
            return G
        else:
            return networkgeneration()



    opinionbanana=[]
    initial_opinion = []
    datas = []



    def Control_Centrality(F):
        length = len(F.nodes)
        i = 1

        def CC(F):
            pippa = F.out_degree()
            pippa1 = np.array(pippa)
            pippa2 = pippa1[pippa1[:, 1] != 0]
            F = F.subgraph(pippa2[:, 0])
            return F

        def layer(F):
            culo = [x for x in F.nodes if x not in CC(F).nodes]
            banana = [i] * len(culo)
            results = np.column_stack((culo, banana))
            return results

        results = layer(F)
        while len(results) < length:
            results = np.concatenate([results, layer(F)])
            results = np.unique(results, axis=0)
            i += 1
            F = CC(F)
            layer(F)
        return dict(zip(results[:, 0], (results[:, 1])))

    def tildealt():
        pl = 0
        thr = 0


        def tilde():

            G=networkgeneration()
            num = len(list(G.nodes))

            cliques = list(nx.enumerate_all_cliques(G))
            triads = [i for i in cliques if len(i) == 3]
            simm = []

            def combine(arr, s):
                return list(combinations(arr, s))

            for n in triads:
                for l in combine(n, 2):
                    simm.append(l)

            simmelian = []
            for i in simm:
                if i not in simmelian:
                    simmelian.append(i)

            simmeliansubj=[]
            for n in simmelian:
                if n[0] not in simmeliansubj:
                    simmeliansubj.append(n[0])
                if n[1] not in simmeliansubj:
                    simmeliansubj.append(n[1])

            issimmelian=[]
            for n in range(num):
                if n in simmeliansubj:
                    issimmelian.append(1)
                else:
                    issimmelian.append(0)

            underdog = []
            for i in G.nodes():
                degreeneighbors = []
                for j in G.neighbors(i):
                    degreeneighbors.append(nx.degree_centrality(G)[j])
                underdog.append(nx.degree_centrality(G)[i] / np.mean(degreeneighbors))

            nx.set_node_attributes(G, 0, name='threshold')
            for n in G.nodes:
                G.nodes[n]['threshold'] = thr
            F = ultravertical

            BC = Control_Centrality(F)

            nx.set_node_attributes(G, 0, name='relevance')

            for n in G.nodes:
                G.nodes[n]['relevance'] = np.random.beta(1, 2)
                initial_opinion.append(G.nodes[n]['relevance'])

            influence_matrix = np.matrix(np.zeros((num, num)))
            for i in G.nodes:
                influence_matrix[i, i] = 0.1
                for j in G.neighbors(i):
                    if [i, j] in simmelian:
                        if BC[i] - BC[j] < 0:
                            influence_matrix[i, j] = 0.8
                        elif BC[i] - BC[j] > 0:
                            influence_matrix[i, j] = 0.8
                        elif BC[i] - BC[j] == 0:
                            influence_matrix[i, j] = 0.8
                    elif [i, j] not in simmelian:
                        if BC[i] - BC[j] > 0:
                            influence_matrix[i, j] = 0.8
                        elif BC[i] - BC[j] < 0:
                            influence_matrix[i, j] = 0.8
                        elif BC[i] - BC[j] == 0:
                            influence_matrix[i, j] = 0.8

            influence_matrix_correct = np.matrix(np.zeros((num, num)))
            for i in range(num):
                for j in range(num):
                    influence_matrix_correct[i, j] = influence_matrix[i, j] / np.sum(influence_matrix[i])

            probability_matrix = np.matrix(np.zeros((num, num)))
            for i in G.nodes:
                for j in G.neighbors(i):
                    if i == j:
                        probability_matrix[i, j] = 1
                    else:
                        probability_matrix[i, j] = 0.9


            datoni = []
            K = G.copy()
            t = 0
            influence_matrices = []

            def event():

                P = K.copy()

                influence_matrix_real = np.matrix(np.zeros((num, num)))
                influence_matrix_tilde = np.matrix(np.zeros((num, num)))
                for i in K.nodes:
                    influence_matrix_real[i, i] = influence_matrix[i, i]
                    for j in K.neighbors(i):
                        if random.random() < probability_matrix[i, j]:
                            if P.nodes[j]['relevance'] > P.nodes[j]['threshold']:
                                influence_matrix_real[i, j] = influence_matrix[i, j]
                            else:
                                influence_matrix_real[i, j] = 0
                        else:
                            influence_matrix_real[i, j] = 0
                for i in K.nodes:
                    influence_matrix_tilde[i, i] = influence_matrix_real[i, i] / np.sum(influence_matrix_real[i],
                                                                                        axis=1)
                    for j in K.nodes:
                        influence_matrix_tilde[i, j] = influence_matrix_real[i, j] / np.sum(influence_matrix_real[i],
                                                                                            axis=1
                                                                                            )
                influence_matrices.append(influence_matrix_tilde)
                for i in K.nodes:
                    opchange = [P.nodes[i]['relevance'] * influence_matrix_tilde[i, i]]
                    for j in K.neighbors(i):
                        opchange.append(influence_matrix_tilde[i, j] * P.nodes[j]['relevance'])
                    K.nodes[i]["relevance"] = np.sum(opchange)
                return K

            while t != steps:
                datelli = []
                datini = []
                t += 1
                event()
                for n in K.nodes:
                    datelli.append(K.nodes[n]['relevance'])
                    if K.nodes[n]['relevance'] > K.nodes[n]['threshold']:
                        datini.append(n)
                pd.DataFrame({'x': datelli})
                datoni.append(datelli)
            """
            plt.plot(datoni, 'k', alpha=0.1)
            plt.title('DeGroot (classic)')

            plt.plot(np.mean(datoni, axis=1), 'r--', label='mean')
            plt.plot(np.median(datoni, axis=1), 'b', label='median')
            plt.legend()
            plt.grid()
            plt.show()
            """

            def matrixMul(a, n):
                if (n <= 1):
                    return a
                else:
                    return np.matmul(matrixMul(a, n - 1), a)

            def matrixMultt(a, n):
                tempArr = a
                for i in range(0, n):
                    tempArr = influence_matrices[i] @ tempArr
                return tempArr


            plt.clf()


            datas.append(np.array(matrixMultt(influence_matrices[0], steps)[1][0]))

            banana=matrixMultt(influence_matrices[0], steps)[1][0].tolist()



            for i in range(num):
                parameters=[]
                parameters.append(matrixMultt(influence_matrices[0], steps)[1][0].tolist()[0][i])
                parameters.append((matrixMultt(influence_matrices[0], steps)[1][0].tolist()[0][i])*num)
                parameters.append(nx.betweenness_centrality(G)[i])
                parameters.append(nx.eigenvector_centrality(G)[i])
                parameters.append(nx.degree_centrality(G)[i])
                parameters.append(nx.closeness_centrality(G)[i])
                parameters.append(nx.pagerank(G)[i])
                parameters.append(issimmelian[i])
                parameters.append(underdog[i])
                influence.append(parameters)
            X_axis = np.arange(len(G.nodes))


            plt.bar(X_axis + 0.2, np.asarray(matrixMul(influence_matrix_correct, steps))[1], 0.4, alpha=0.7,
                    color='blue', label='without randomness')
            plt.bar(X_axis - 0.2, np.asarray(matrixMultt(influence_matrices[0], steps))[1], 0.4, alpha=0.7, color='red',
                    label='with randomness')

            plt.legend()


        while pl != samples:
            print(100*(pl/samples),'%')
            pl += 1
            tilde()
    tildealt()




    return influence



banana=test()




dfs = [pd.DataFrame({k: sample}) for k, sample in enumerate(banana)]


df = pd.concat(dfs, ignore_index=True, axis=1)
df = df.T
df = df.rename(columns={0: 'influence', 1: 'influence (norm)', 2: 'betweenness', 3:'eigenvector',4:'degree',
                        5:'closeness',6:'pagerank',7:'simmelian',8:'underdog'})


x= df[df.columns[2:]].to_numpy()
x = sm.add_constant(x)
y=df['influence (norm)'].to_numpy()
y1=df['influence'].to_numpy()

model = sm.OLS(y, x)
results = model.fit()
print(results.summary())


library(readr)
library(wrapr)
library(tidyverse)
library(sna)
library(igraph)
library(ergm)
library(haven)


data <- read_excel("C:/Users/Manag22/PycharmProjects/pythonProject/6x6notvertical03lim.xlsx", 
                   +     col_types = c("skip", "numeric", "numeric", 
                                       +         "numeric", "numeric"))

data<-X6x6notvertical01lim

colnames(data) <- c("ns4", "ns5","S4","S5")

dati<-data %>% gather(group, influence)
tapply(dati$influence,dati$group,mean)
tapply(dati$influence,dati$group,var)
ggplot(aes(x=group,y=influence),data=dati)+geom_boxplot()+theme_bw()

mod1<-aov(influence ~ group, data=dati,contrasts = list(group="contr.sum"))
plot(mod1)
summary.lm(mod1)

intercept<-mean(dati$influence)
intercept

alphas<-tapply(dati$influence,dati$group,mean)-mean(dati$influence)
alphas



TukeyHSD(mod1)



kruskal.test(influence ~ group, data = dati)

pairwise.wilcox.test(dati$influence,dati$group,p.adjust.method = 'bonferroni')
