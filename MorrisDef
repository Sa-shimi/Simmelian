import networkx as nx
from SALib.analyze import sobol
import numpy as np
import string
import pandas as pd
import itertools
from itertools import repeat
from networkx.utils import not_implemented_for
from itertools import combinations, permutations
from collections import defaultdict
from random import sample
from collections import deque
from itertools import chain
from itertools import islice
from networkx.utils import not_implemented_for
import statistics
from random import randrange
import time
import threading
from SALib.sample import morris as morris
from SALib.analyze.morris import analyze
import telebot
from itertools import combinations
import statistics
from random import randrange
from SALib.analyze.sobol import analyze
from SALib.analyze.morris import analyze


def test():
    p = randrange(13,16)
    num = randrange(40,100)
    F = nx.random_tree(num)

    G = nx.barabasi_albert_graph(num, p)

    def combine(arr, s): 
        return list(combinations(arr, s))

    def Control_Centrality(F):
        length = len(F.nodes)
        i = 1

        def CC(F):
            pippa = F.out_degree()
            pippa1 = np.array(pippa)
            pippa2 = pippa1[pippa1[:, 1] != 0]
            F = F.subgraph(pippa2[:, 0])
            return F

        def layer(F):
            culo = [x for x in F.nodes if x not in CC(F).nodes]
            banana = [i] * len(culo)
            results = np.column_stack((culo, banana))
            return results

        results = layer(F)

        while len(results) < length:
            results = np.concatenate([results, layer(F)])
            results = np.unique(results, axis=0)
            i += 1
            F = CC(F)
            layer(F)
        return dict(zip(results[:,0], (results[:,1]/max(results[:,1]))))

    BC = nx.betweenness_centrality(F)

                
            

    isinstance(BC, dict)
    list(G.nodes)

    lll = combinations(G.nodes(), 2)
    lll = np.matrix(list(lll))
    cliques = list(nx.enumerate_all_cliques(G))
    triads = [i for i in cliques if len(i) == 3]
    simm= []
    for n in triads:
        for l in combine(n,2):
            simm.append(l)


    simmelian = []
    for i in simm:
        if i not in simmelian:
            simmelian.append(i)

    simmelian=np.array(simmelian)

    A= simmelian[:,0]
    B= simmelian[:,1]
    A1 = lll[:, 0]
    B1 = lll[:, 1]
    df=pd.DataFrame(simmelian)
    numsimm = len(simmelian)

    BCA = []
    for x in (A):
        for n in (BC):
            if x == n:
                BCA.append(BC[n])
    BCB = []
    for y in (B):
        for n in (BC):
            if y == n:
                BCB.append(BC[n])


    df['BCA'] = BCA
    df['BCB'] = BCB
    df['AVG'] = (df['BCA'] + df['BCB']) / 2
    df['VAR'] = (((df['BCA'] - df['AVG']) ** 2 + (df['BCB'] - df['AVG']) ** 2) / 2)
    VAR = np.sum(df['VAR'])
    V = VAR / numsimm
    df1 = df[3:4]
    dff = []
    for k in G.nodes:
        for n in (BC):
            if k == n:
                dff.append(BC[n])
    T = max(dff)

    H = pd.DataFrame(dff)
    H['BCMax'] = T
    H.rename(columns={0: 'BCentrality'}, inplace=True)
    H['Hier'] = (H['BCMax'] - H['BCentrality'])
    Q = np.mean(H['Hier'])
    v = Q * V

    BC1 = nx.betweenness_centrality(F)
    isinstance(BC1, dict)
    df1 = pd.DataFrame(lll)
    BC1A = []
    for x in (A1):
        for n in (BC1):
            if x == n:
                BC1A.append(BC1[n])
    BC1B = []
    for y in (B1):
        for n in (BC1):
            if y == n:
                BC1B.append(BC1[n])
    df1['BC1A'] = BC1A
    df1['BC1B'] = BC1B
    df1['AVG1'] = (df1['BC1A'] + df1['BC1B']) / 2
    df1['VAR1'] = (((df1['BC1A'] - df1['AVG1']) ** 2 + (df1['BC1B'] - df1['AVG1']) ** 2)) / 2
    V1 = np.mean(df1['VAR1'])
    df11 = df1[3:4]
    dff1 = []
    for k in G.nodes:
        for n in (BC1):
            if k == n:
                dff1.append(BC1[n])
    T1 = max(dff1)
    H1 = pd.DataFrame(dff1)
    H1['BC1Max'] = T1
    H1.rename(columns={0: 'BCentrality1'}, inplace=True)
    H1['Hier1'] = (H1['BC1Max'] - H1['BCentrality1'])
    Vl = V / V1
    e = len(list(G.edges))
    Density = (2 * e * ((num * (num - 1)) ** -1))
    Clustering = nx.average_clustering(G)

    def eccentricity(G):
        n = len(G)
        if n == 0:
            msg = (
                "the null graph has no paths, thus there is no average"
                "shortest path length")
            raise nx.NetworkXPointlessConcept(msg)
        if n == 1:
            return 0
        if G.is_directed() and not nx.is_weakly_connected(G):
            raise nx.NetworkXError("Graph is not weakly connected.")
        if not G.is_directed() and not nx.is_connected(G):
            raise nx.NetworkXError("Graph is not connected.")

        def path_length(v):
            return nx.single_source_shortest_path_length(G, v)

        s = statistics.variance(l for u in G for l in path_length(u).values())
        return s

    P = np.matrix([Vl, Q, eccentricity(G), Density, numsimm, len(lll), e, p])

    return (P)


num_vars = 7

problem = {
    'num_vars': 7,
    'names': ['H', 'EX', 'D', 'numsimm', 'num potential simmdya', 'Edges', 'p'],
    'groups': None
}

param_values = np.ndarray(shape=((num_vars + 1),), dtype=float)
for _ in range(100 * (num_vars + 1)):
    test()
    param_values = np.vstack([param_values, test()])

param_values = np.delete(param_values, 0, 0)
Y = np.array(param_values[:, 0])
Y = Y.flatten()
param_values = np.delete(param_values, 0, 1)
param_values = np.array(param_values)
Si = analyze(problem, param_values, Y, num_resamples=100, print_to_console=True)

